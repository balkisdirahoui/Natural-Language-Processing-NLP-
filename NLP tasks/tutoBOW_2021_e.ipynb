{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 1 : Classics\n",
    "\n",
    "In this practical session, we review some of the basics of text classification: **bag of words and linear models**\n",
    "\n",
    "## Classic NLP pipeline\n",
    "\n",
    "To do classification on text using statistical classifiers it is mandatory to vectorize text. Different methods exists to go from raw text to vectors. In this practical session we propose to explore the bag of word model.\n",
    "\n",
    "### Bag of Word model:\n",
    "\n",
    "- A dictionnary of all considered word is built (of size $D$) from training text. Creating this dictionnary can be seen as handcrafting features.\n",
    "- Each document is represented as a **sparse** vector (of size $D$) coding for each possible word(features) in the dictionnary. \n",
    "\n",
    "\n",
    "### Classification pipeline overview\n",
    "\n",
    "```\n",
    "      \n",
    "raw text ---|Feature extraction|---> vectors ---|statistical classifier|---> pos or neg\n",
    "```\n",
    "\n",
    "In this pipeline you have two ways of improving performance:\n",
    "\n",
    "- handcraft better features\n",
    "- optimizing the classifier hyper-parameters\n",
    "\n",
    "\n",
    "### Goal of this session:\n",
    "\n",
    "1. Warmup (just read and run -- make sure you know what's happening)\n",
    "2. Tinker with the feature extractor and try different methods (stop-words, stemming,...)\n",
    "3. Tinker with the classifiers\n",
    "4. Visualize best features\n",
    "5. **(Bonus)** make word clouds of those features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data: Large Movie Review Dataset \n",
    "\n",
    "Here, to explore this classical NLP pipeline, we propose to do binary sentiment classification.  For this, we use a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. \n",
    "\n",
    "- [The original dataset can be found here](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "\n",
    "To make things easier, the data was already formatted into a json (`json_pol`) containing a train and a test list of `(string(review),int(class))` couples. As it's a binary classification problem, there are two classes: \n",
    "\n",
    "- `0` codes for negative reviews \n",
    "- `1` codes for positive reviews\n",
    "\n",
    "\n",
    "The json has the following format: \n",
    "```json\n",
    "{\n",
    "\"train\":[[\"this is a positive review\",1],[\"this is a negative review\",0],...],\n",
    "\"test\":[[\"this is a positive review\",1],[\"this is a negative review\",0],...]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------\n",
    "# WARMUP (just read and run)\n",
    "## Step 1: Load Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> /!\\ YOU NEED TO UNZIP dataset/json_pol.zip first /!\\</font>\n",
    "\n",
    "The json of the following format: `{\"train\":[[review,class],...], \"test\":[[review,class],...]}`. \n",
    "\n",
    " - We need to load it and collect both test and train lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "[\"The undoubted highlight of this movie is Peter O'Toole's performance. In turn wildly comical and terribly terribly tragic. Does anybody do it better than O'Toole? I don't think so. What a great face that man has!<br /><br />The story is an odd one and quite disturbing and emotionally intense in parts (especially toward the end) but it is also oddly touching and does succeed on many levels. However, I felt the film basically revolved around Peter O'Toole's luminous performance and I'm sure I wouldn't have enjoyed it even half as much if he hadn't been in it.\", 1]\n",
      "\n",
      "Number of test reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "['Although credit should have been given to Dr. Seuess for stealing the story-line of \"Horton Hatches The Egg\", this was a fine film. It touched both the emotions and the intellect. Due especially to the incredible performance of seven year old Justin Henry and a script that was sympathetic to each character (and each one\\'s predicament), the thought provoking elements linger long after the tear jerking ones are over. Overall, superior acting from a solid cast, excellent directing, and a very powerful script. The right touches of humor throughout help keep a \"heavy\" subject from becoming tedious or difficult to sit through. Lastly, this film stands the test of time and seems in no way dated, decades after it was released.', 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "#### /!\\ YOU NEED TO UNZIP dataset/json_pol.zip first /!\\\n",
    "\n",
    "\n",
    "# Loading json\n",
    "with open(\"dataset/json_pol\",encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "    json_data = json.loads(data[0])\n",
    "    train = json_data[\"train\"]\n",
    "    test = json_data[\"test\"]\n",
    "    \n",
    "\n",
    "# Quick Check\n",
    "counter_train = Counter((x[1] for x in train))\n",
    "counter_test = Counter((x[1] for x in test))\n",
    "print(\"Number of train reviews : \", len(train))\n",
    "print(\"----> # of positive : \", counter_train[1])\n",
    "print(\"----> # of negative : \", counter_train[0])\n",
    "print(\"\")\n",
    "print(train[0])\n",
    "print(\"\")\n",
    "print(\"Number of test reviews : \",len(test))\n",
    "print(\"----> # of positive : \", counter_test[1])\n",
    "print(\"----> # of negative : \", counter_test[0])\n",
    "\n",
    "print(\"\")\n",
    "print(test[0])\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, we need to vectorize the text so it can be used by classifiers.\n",
    "Different methods exists to vectorize text. Here we use a [bag of word](https://en.wikipedia.org/wiki/Bag-of-words_model) approach:\n",
    "\n",
    ">  In the bag of word model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.\n",
    "\n",
    "In other words, each text becomes a (sparse) vector which codes for its words. With the following function from scikit-learn, it is straightforward to get a bag of word from raw texts: [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "\n",
    ">Convert a collection of text documents to a matrix of token counts\n",
    "This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data. [USER GUIDE](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02', '020410', '029', '03', '04', '041', '05', '050', '06', '06th', '07', '08', '087', '089', '08th', '09', '0f', '0ne', '0r', '0s', '10', '100', '1000', '1000000', '10000000000000', '1000lb', '1000s', '1001', '100b', '100k', '100m', '100min', '100mph', '100s', '100th', '100x', '100yards', '101', '101st', '102', '102nd', '103', '104', '1040', '1040a', '1040s', '105', '1050', '105lbs', '106', '106min', '107', '108', '109', '10am', '10lines', '10mil', '10min', '10minutes', '10p', '10pm', '10s', '10star', '10th', '10x', '10yr', '11', '110', '1100', '11001001', '1100ad', '111', '112', '1138', '114', '1146', '115', '116', '117', '11f', '11m', '11th', '12', '120', '1200', '1200f', '1201', '1202', '123', '12383499143743701', '125', '125m', '127', '128', '12a', '12hr', '12m', '12mm', '12s', '12th', '13', '130', '1300', '1300s', '131', '1318', '132', '134', '135', '135m', '136', '137', '138', '139', '13k', '13s', '13th', '14', '140', '1408', '140hp', '1415', '142', '145', '1454', '146', '147', '1473', '149', '1492', '14a', '14ieme', '14s', '14th', '14yr', '14Ã¨me', '15', '150', '1500', '1500s', '150_worst_cases_of_nepotism', '150k', '150m', '151', '152', '153', '1547', '155', '156', '1561', '157', '158', '1594', '15mins', '15minutes', '15s', '15th', '16', '160', '1600', '1600s', '160lbs', '161', '1610', '163', '164', '165', '166', '1660s', '168', '169', '1692', '16ieme', '16k', '16mm', '16s', '16th', '16x9', '16Ã¨me', '16Ã©me', '17', '170', '1700', '1700s', '1701', '171', '175', '177', '1775', '1780s', '1790s', '1794', '1798', '17million', '17th', '18', '180', '1800', '1800mph', '1800s', '1801', '1805', '1809', '180d', '1812', '1813', '18137', '1814', '1816', '1820', '1824', '183', '1830', '1832', '1836', '1837', '1838', '1839', '1840', '1840s', '1844', '1846', '1847', '185', '1850', '1850ies', '1850s', '1852', '1853', '1854', '1855', '1859', '1860', '1860s', '1861', '1862', '1863', '1864', '1865', '1870', '1870s', '1871', '1873', '1874', '1875', '1876', '188', '1880', '1880s', '1881', '1886', '1887', '1888', '1889', '188o', '1890', '1890s', '1892', '1893', '1894', '1895', '1896', '1897', '1898', '1899', '18a', '18s', '18th', '18year', '19', '190', '1900', '1900s', '1901', '1902', '1903', '1904', '1905', '1906', '1907', '1908', '1909', '1910', '1910s', '1911', '1912', '1913', '1914', '1915', '1916', '1917', '1918', '1919', '192', '1920', '1920ies', '1920s', '1921', '1922', '1923', '1924', '1925', '1926', '1927', '1928', '1929', '1930', '1930ies', '1930s', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939', '193o', '194', '1940', '1940s', '1941', '1942', '1943', '1944', '1945', '1946', '1947', '1948', '1949', '1949er', '195', '1950', '1950s', '1951', '1952', '1953', '1954', '1955', '1956', '1957', '1958', '1959', '1960', '1960s', '1961', '1961s', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '197', '1970', '1970ies', '1970s', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '19796', '197o', '1980', '1980ies', '1980s', '1981', '1982', '1982s', '1983', '1983s', '1984', '1984ish', '1985', '1986', '1987', '1988', '1989', '1990', '1990s', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '19k', '19th', '19thc', '1am', '1and', '1d', '1h', '1h30', '1h40', '1h40m', '1h53', '1hour', '1hr', '1million', '1min', '1mln', '1o', '1s', '1st', '1ton', '1tv', '1Â½', '1Ã§', '20', '200', '2000', '20000', '20001', '2000ad', '2000s', '2001', '2002', '2003', '2004', '2004s', '2005', '2006', '2007', '2008', '2009', '200ft', '200th', '201', '2010', '2012', '2013', '2015', '2017', '2019', '2020', '2022', '2023', '2030', '2031', '2033', '2035', '2036', '2038', '204', '2040', '2044', '2046', '2047', '2050', '2053', '2054', '206', '2060', '2070', '2080', '209', '2090', '20c', '20ft', '20k', '20m', '20mins', '20minutes', '20mn', '20p', '20perr', '20s', '20th', '20ties']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "classes = [pol for text,pol in train]\n",
    "corpus = [text for text,pol in train]\n",
    "\n",
    "# vectorizer = CountVectorizer(input='content', encoding='utf-8',\n",
    "#                              decode_error='strict', strip_accents=None,\n",
    "#                              lowercase=True, preprocessor=None, tokenizer=None,\n",
    "#                              stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b',\n",
    "#                              ngram_range=(1, 1), analyzer='word',\n",
    "#                              max_df=1.0, min_df=1, max_features=None,\n",
    "#                              vocabulary=None, binary=False, dtype='numpy.int64')\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names()[:500]) # we only print a few\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Classifiers\n",
    "\n",
    "Once we have vectorized data, we can use them to train statistical classifiers.\n",
    "\n",
    "Here, we propose to use three classic options:\n",
    "\n",
    "- NaÃ¯ve bayes\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "\n",
    "\n",
    "We fit each model below with default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we evaluate the accuracy of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaÃ¯ve Bayes accuracy: 0.81356\n",
      "Logistic Regression accuracy: 0.86372\n",
      "SVM accuracy: 0.84572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#NaÃ¯ve Bayes\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X, classes)\n",
    "\n",
    "\n",
    "#Logistic Regression\n",
    "lr_clf = LogisticRegression(random_state=0, solver='lbfgs',n_jobs=-1)\n",
    "lr_clf.fit(X, classes)\n",
    "\n",
    "#Linear SVM\n",
    "svm_clf = LinearSVC(random_state=0, tol=1e-5)\n",
    "svm_clf.fit(X, classes)\n",
    "\n",
    "\n",
    "true = [pol for text,pol in test]\n",
    "test_corpus = [text for text,pol in test]\n",
    "X_test = vectorizer.transform(test_corpus)\n",
    "\n",
    "pred_nb = nb_clf.predict(X_test)\n",
    "pred_lr = lr_clf.predict(X_test)\n",
    "pred_svm = svm_clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(f\"NaÃ¯ve Bayes accuracy: {accuracy_score(true, pred_nb)}\")\n",
    "print(f\"Logistic Regression accuracy: {accuracy_score(true, pred_lr)}\")\n",
    "print(f\"SVM accuracy: {accuracy_score(true, pred_svm)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------\n",
    "\n",
    "# EXERCISES (now it's your turn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further improve performances, we can try to improve the simple bag of word model as used above to address some issues: \n",
    " \n",
    "\n",
    "- **(a)**  If we visualize the word frequency distribution we see that a few words (roughly 20) appear a lot more than the others. These words are often refered to as **stop words**. Would remove them improve accuracy ?\n",
    "- **(b)** Some words are made of two words or three...\n",
    "- **(c)** Is it really necessary to have words which appears only in 2 or 3 reviews ? \n",
    "- **(d)** Punctuation, UPPER CASE, numbers... Do cleaning bring improvements?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's plot the count of the 1000 most used words:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "wc = Counter()\n",
    "for text,pol in train+test:\n",
    "    wc.update(text.split(\" \"))\n",
    "    \n",
    "freq = [f for w,f in wc.most_common(1000)]\n",
    "\n",
    "plt.plot(freq[:1000])\n",
    "print(wc.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) let's remove stopwords**\n",
    "\n",
    ">stop_words : string {â€˜englishâ€™}, list, or None (default)\n",
    "\n",
    ">If â€˜englishâ€™, a built-in stop word list for English is used. There are several known issues with â€˜englishâ€™ and you should consider an alternative (see Using stop words).\n",
    "If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word'.\n",
    "If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms.\n",
    "\n",
    "[see CountVectorizer for full doc](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CountVectorizer can take a list of stop words as argument.\n",
    "# Build or download a list of stop word (from NLTK for exemple)\n",
    "\n",
    "stop_words = [\"the\", \"a\", \"and\"] #Make a better list\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) n-grams**\n",
    "\n",
    ">ngram_range : tuple (min_n, max_n)\n",
    "The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.\n",
    "\n",
    ">analyzer : string, {â€˜wordâ€™, â€˜charâ€™, â€˜char_wbâ€™} or callable\n",
    "Whether the feature should be made of word or character n-grams. Option â€˜char_wbâ€™ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.\n",
    "If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.\n",
    "\n",
    "[see CountVectorizer for full doc](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,1),analyzer='word') # Maybe 2-grams or 3-grams bring improvements ?\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Restrict dictionnary size**\n",
    "\n",
    ">max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    ">min_df : float in range [0.0, 1.0] or int, default=1\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    ">max_features : int or None, default=None\n",
    "If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "[see CountVectorizer for full doc](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_df=100000,min_df=5,max_features=25000) #try out some values\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "#What is the dictionnary size now ?\n",
    "dic_size = ###\n",
    "print(dic_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Clean text ?**\n",
    ">strip_accents : {â€˜asciiâ€™, â€˜unicodeâ€™, None}\n",
    "Remove accents and perform other character normalization during the preprocessing step. â€˜asciiâ€™ is a fast method that only works on characters that have an direct ASCII mapping. â€˜unicodeâ€™ is a slightly slower method that works on any characters. None (default) does nothing.\n",
    "Both â€˜asciiâ€™ and â€˜unicodeâ€™ use NFKD normalization from unicodedata.normalize.\n",
    "\n",
    ">lowercase : boolean, True by default\n",
    "Convert all characters to lowercase before tokenizing.\n",
    "\n",
    ">preprocessor : callable or None (default)\n",
    "Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps.\n",
    "\n",
    ">token_pattern : string\n",
    "Regular expression denoting what constitutes a â€œtokenâ€, only used if analyzer == 'word'. The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\n",
    "\n",
    "\n",
    "`Prepocessor` argument takes a function which processes text directly. This way it becomes easy to do \"fancy\" things like:\n",
    "- [part of speech tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging)\n",
    "- [stemming](https://en.wikipedia.org/wiki/Stemming) \n",
    "\n",
    "To do both, you can use [NLTK tagger](http://www.nltk.org/api/nltk.tag.html) and [NLTK stemmer](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem).\n",
    "\n",
    "\n",
    "[see CountVectorizer for full doc](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "reg = \"\\b[^\\W]\\b\" #matches word with characters \n",
    "\n",
    "# 1) Try removing punctuation or putting text to lower case (maybe use a regex)\n",
    "# 2) Try \"Stemming\" - \"pos-tagging\" the text\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Transforms text to remove unwanted bits.\n",
    "    \"\"\"\n",
    "    return text.replace(\".\",\" \") # This function is only taking care of dots, what about !:,?+-&*%\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=preprocess)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(bonus)** \n",
    "\n",
    "- Instead of word counts, the bag of word vector can only represent used word:\n",
    "\n",
    "> binary : boolean, default=False\n",
    "If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.\n",
    "\n",
    "[see CountVectorizer for full doc](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "\n",
    "- Words can also be weighted by importance. The most used weighting scheme in Information Retrieval is TF-IDF.\n",
    "\n",
    "[TfidfVectorizer from scikit can be directly used](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#vectorizer = CountVectorizer(binary=True)\n",
    "#vectorizer = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer the following questions\n",
    "\n",
    "- What is the most effective pre-processing ?\n",
    "- Which model is the most accurate ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#NaÃ¯ve Bayes\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X, classes)\n",
    "\n",
    "#Logistic Regression\n",
    "lr_clf = LogisticRegression(random_state=0, solver='lbfgs',n_jobs=-1)\n",
    "lr_clf.fit(X, classes)\n",
    "\n",
    "#Linear SVM\n",
    "svm_clf = LinearSVC(random_state=0, tol=1e-5)\n",
    "svm_clf.fit(X, classes)\n",
    "\n",
    "\n",
    "true = [pol for text,pol in test]\n",
    "test_corpus = [text for text,pol in test]\n",
    "X_test = vectorizer.transform(test_corpus)\n",
    "\n",
    "pred_nb = nb_clf.predict(X_test)\n",
    "pred_lr = lr_clf.predict(X_test)\n",
    "pred_svm = svm_clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(f\"NaÃ¯ve Bayes accuracy: {accuracy_score(true, pred_nb)}\")\n",
    "print(f\"Logistic Regression accuracy: {accuracy_score(true, pred_lr)}\")\n",
    "print(f\"SVM accuracy: {accuracy_score(true, pred_svm)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing features\n",
    "\n",
    "It can be interesting to find out which words are the most positive or negative for our models. To do so, you can simply look how each models weigth each features with respect to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we first build a dictionnary {id_feature : word} from our vectorizer\n",
    "\n",
    "features = {v:k for k,v in vectorizer.vocabulary_.items()} # invert mapping (k2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaÃ¯ve Bayes\n",
    "\n",
    "For the naÃ¯ve bayes model, we can look directly at `p(word | class)`\n",
    "\n",
    "[MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)\n",
    "\n",
    "> feature_log_prob_ : array, shape (n_classes, n_features)\n",
    "Empirical log probability of features given a class, P(x_i|y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#NaÃ¯ve bayes\n",
    "\n",
    "k = 50 # we want the 50 most negative and positive words\n",
    "\n",
    "feat_neg = nb_clf.feature_log_prob_[0] # get list of negative class log probability\n",
    "#feat_pos =  # same for positive\n",
    "\n",
    "most_neg = [] # find the corresponding words\n",
    "most_pos = [] \n",
    "\n",
    "print(most_neg[:k])\n",
    "print(most_pos[:k])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models : Logistic Regression & SVM\n",
    "\n",
    "\n",
    "For linear models, we can look at feature coefficients:\n",
    " [Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    " \n",
    " \t\n",
    ">coef_ : array, shape (1, n_features) or (n_classes, n_features)\n",
    "Coefficient of the features in the decision function.\n",
    "coef_ is of shape (1, n_features) when the given problem is binary. In particular, when multi_class=â€™multinomialâ€™, coef_ corresponds to outcome 1 (True) and -coef_ corresponds to outcome 0 (False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "k = 50 # we want the 50 most negative and positive words\n",
    "\n",
    "feat = lr_clf.coef_[0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SVM\n",
    "k = 50 # we want the 50 most negative and positive words\n",
    "\n",
    "feat = svm_clf.coef_[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "------------------------\n",
    "##Â **(bonus:)**  [make word clouds !](https://github.com/amueller/word_cloud)\n",
    "\n",
    "### Installation\n",
    "If you are using pip:\n",
    "\n",
    "`pip install wordcloud`\n",
    "\n",
    "### If you are using conda, you can install from the conda-forge channel:\n",
    "\n",
    "`conda install -c conda-forge wordcloud`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import ...\n",
    "\n",
    "# Get text:\n",
    "#text = \"this is exemple text\"\n",
    "\n",
    "# Generate a word cloud image\n",
    "# wordcloud = WordCloud().generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------ End of Practical -------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy loading method (if you use original data)\n",
    "\n",
    "Data is split into two folders \"train\" and \"test\" and in each of these folders is two other folders \"neg\" and \"pos\" containing reviews in \".txt\". One \".txt\" per review\n",
    "\n",
    "=> We wish to load those reviews `(text,polarity)` in two lists: `train` and `test`\n",
    "\n",
    "Data format is: numReview_polarity.txt , one file per review.\n",
    "\n",
    "Now that we have each review's filepath we have two things left to do:\n",
    " - read each review from file \n",
    " - extract polarity from filename\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### This code is for legacy\n",
    "\n",
    "def get_polarity(f):\n",
    "    \"\"\"\n",
    "    Extracts polarity from filename:\n",
    "    0 is negative (< 5)\n",
    "    1 is positive (> 5)\n",
    "    \"\"\"\n",
    "    _,name = pathsplit(f)\n",
    "    if int(name.split('_')[1].split('.')[0]) < 5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def open_one(f):\n",
    "    \"\"\"\n",
    "    open one file, gets polarity and review text\n",
    "    \"\"\"\n",
    "    polarity = get_polarity(f)\n",
    "    \n",
    "    with open(f,\"r\",encoding='utf-8') as review:\n",
    "        text = \" \".join(review.readlines()).strip()\n",
    "    \n",
    "    return (text,polarity)\n",
    "\n",
    "\n",
    "import glob\n",
    "from os.path import split as pathsplit\n",
    "\n",
    "#We first recover all reviews filepaths using glob - https://docs.python.org/3/library/glob.html\n",
    "\n",
    "dir_train = \"dataset/aclImdb/train/\"\n",
    "dir_test = \"dataset/aclImdb/test/\"\n",
    "\n",
    "train_files = glob.glob(dir_train+'pos/*.txt') + glob.glob(dir_train+'neg/*.txt')\n",
    "test_files = glob.glob(dir_test+'pos/*.txt') + glob.glob(dir_test+'neg/*.txt')\n",
    "\n",
    "\n",
    "print(train_files[:3]) #We have 2 lists of files\n",
    "\n",
    "print(open_one(train_files[0])) #test\n",
    "\n",
    "train = [open_one(x) for x in train_files]\n",
    "test = [open_one(x) for x in test_files]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}